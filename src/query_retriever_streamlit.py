import os
import streamlit as st
from langchain import hub
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from dotenv import load_dotenv
import math

#########################
# ìœ ì‚¬ë„ ì„ê³„ì¹˜(Threshold)
#########################
SIMILARITY_THRESHOLD = 0.6  # ì´ ê°’ ì´ìƒì¸ ë¬¸ì„œë§Œ ì¶œì²˜ë¡œ í‘œì‹œ

# ê°„ë‹¨í•œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° í•¨ìˆ˜
def cosine_similarity(vec_a, vec_b):
    dot = sum(a * b for a, b in zip(vec_a, vec_b))
    norm_a = math.sqrt(sum(a * a for a in vec_a))
    norm_b = math.sqrt(sum(b * b for b in vec_b))
    if (norm_a * norm_b) == 0:
        return 0.0
    return dot / (norm_a * norm_b)

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# OpenAI API ì„¤ì •
api_key = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_KEY"] = api_key
if not api_key:
    raise ValueError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")

# í”„ë¡¬í”„íŠ¸ íŒŒì¼ ê²½ë¡œ ì„¤ì •
prompt_file_path = "src/prompt/prompt.txt"

# í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì½ì–´ì™€ í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸ë¡œ ì €ì¥
def load_prompt_from_file(file_path):
    with open(file_path, "r", encoding="utf-8") as file:
        return file.read()

# íŒŒì¼ì—ì„œ í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸ ì½ê¸°
prompt_text = load_prompt_from_file(prompt_file_path)

# ì„ë² ë”© ëª¨ë¸ ìƒì„± - text-embedding-3-small ì‚¬ìš©
embedding_model = OpenAIEmbeddings(model="text-embedding-3-small")

# ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ (allow_dangerous_deserialization ì¸ìë¥¼ ì¶”ê°€)
vectorstore = FAISS.load_local("vdb/faiss_index", embeddings=embedding_model, allow_dangerous_deserialization=True)

# lambda_mult ê°’ì— ë”°ë¥¸ ê²€ìƒ‰ë°©ë²•
# 1.0: ìˆœìˆ˜ ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰(ë‹¤ì–‘ì„± ê³ ë ¤ X) == similarity
# 0.0: ìµœëŒ€ ë‹¤ì–‘ì„± ì¶”êµ¬(ìœ ì‚¬ë„ ê³ ë ¤ X)
# 0.9: ìœ ì‚¬ë„ì— 90% ê°€ì¤‘ì¹˜, ë‹¤ì–‘ì„±ì— 10% ê°€ì¤‘ì¹˜
retriever = vectorstore.as_retriever(
    search_type='mmr',
    search_kwargs={'k': 5, 'fetch_k': 10, 'lambda_mult': 0.9}
)

# RAG êµ¬ì„± ìš”ì†Œ ì„¤ì •
prompt = PromptTemplate(
    input_variables=["question", "context"],  # í•„ìš”í•œ ì…ë ¥ ë³€ìˆ˜ ì„¤ì •
    template=prompt_text  # í…ìŠ¤íŠ¸ íŒŒì¼ì—ì„œ ì½ì–´ì˜¨ í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸
)

llm = ChatOpenAI(model_name="gpt-4o", temperature=0)
rag_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# Streamlit ì•± ì„¤ì •
st.set_page_config(page_title="ì„¸ë¬´ì‚¬ ì±—ë´‡", page_icon="ğŸ¤–", layout="wide")

st.title("ğŸ“„ ì„¸ë¬´ì‚¬ ì±—ë´‡")
st.write("ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ê³  ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.")

# ì‚¬ìš©ì ì…ë ¥ í¼
with st.form("chat_form"):
    question = st.text_input(
        "ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:",
        placeholder="ì˜ˆ: ëŒ€í•™ì›ìƒì¸ ë°°ìš°ìê°€ 2024ë…„ 6ì›”ì— ì—°êµ¬ìš©ì—­ë¹„ë¡œ 500ë§Œì›ì„ ë°›ì€ ê²½ìš° ë°°ìš°ìê³µì œê°€ ê°€ëŠ¥í•´?"
    )
    submit_button = st.form_submit_button(label="ì§ˆë¬¸í•˜ê¸°")

if submit_button and question:
    # ë¬¸ì„œ ê²€ìƒ‰ (MMR ì‚¬ìš©)
    retrieved_documents = retriever.invoke(question)

    # ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ì„ ê²½ìš° ì²˜ë¦¬
    if not retrieved_documents:
        st.warning("ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        # RAGë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„±
        with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
            response = rag_chain.invoke(question)

        # ---------------------------
        # ë‹µë³€ ì¶œë ¥
        # ---------------------------
        st.subheader("ğŸ’¡ ìƒì„±ëœ ë‹µë³€")
        st.write(response)

        # ---------------------------
        # ì¶œì²˜ í‘œì‹œ ì „, ì§ˆë¬¸-ë¬¸ì„œ ìœ ì‚¬ë„ ê³„ì‚°
        # ---------------------------
        with st.spinner("ì¶œì²˜ ë¬¸ì„œ ì ìˆ˜ ê³„ì‚° ì¤‘..."):
            # ì§ˆë¬¸ ì„ë² ë”© ë¯¸ë¦¬ ê³„ì‚°
            question_embedding = embedding_model.embed_query(question)

            # MMRë¡œ ì„ ì •ëœ ë¬¸ì„œë¥¼ ìˆœíšŒí•˜ë©´ì„œ ìœ ì‚¬ë„ ê³„ì‚°
            documents_to_display = []
            for doc in retrieved_documents:
                # ë¬¸ì„œ ë‚´ìš© ì„ë² ë”© (ëŒ€ëµì ìœ¼ë¡œ ë¬¸ì„œ ì „ì²´ë¥¼ queryì²˜ëŸ¼ embed)
                doc_embedding = embedding_model.embed_query(doc.page_content)
                sim = cosine_similarity(question_embedding, doc_embedding)

                # ì„ê³„ì¹˜ ì´ìƒì´ë©´ ì¶œì²˜ë¡œ í‘œì‹œ ëª©ë¡ì— ì¶”ê°€
                if sim >= SIMILARITY_THRESHOLD:
                    documents_to_display.append((doc, sim))

        # ---------------------------
        # ë¦¬íŠ¸ë¦¬ë²„ëœ ë¬¸ì„œ ì¤‘ í•„í„°ë§ëœ ê²ƒë§Œ Expandë¡œ ì¶œë ¥
        # ---------------------------
        st.subheader("ğŸ” ì°¸ì¡°í•œ ë¬¸ì„œ")
        if not documents_to_display:
            st.info(f"**ìœ ì‚¬ë„ {SIMILARITY_THRESHOLD} ì´ìƒ**ì¸ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. (ì¶œì²˜ ì—†ìŒ)")
        else:
            for idx, (doc, sim_score) in enumerate(documents_to_display, 1):
                with st.expander(f"ë¬¸ì„œ {idx}: {doc.metadata.get('ì œëª©', 'ì œëª© ì—†ìŒ')} (ìœ ì‚¬ë„: {sim_score:.3f})"):
                    st.write(f"**ë³¸ë¬¸:** {doc.metadata.get('ë³¸ë¬¸_ì›ë³¸', 'ì—†ìŒ')}")
                    st.write(f"**ì¶œì²˜:** {doc.metadata.get('source', 'ì¶œì²˜ ì—†ìŒ')}")
