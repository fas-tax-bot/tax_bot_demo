import os
import streamlit as st
import numpy as np
import faiss
import pandas as pd
from rank_bm25 import BM25Okapi
from dotenv import load_dotenv

# LangChain / Custom modules
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS


# -----------------------------------------------------------------------------
# 1) ì„¤ì • ìƒìˆ˜ë“¤
# -----------------------------------------------------------------------------
# ALPHA: BM25ì™€ FAISS ê°€ì¤‘ì¹˜ (0=FAISS 100%, 1=BM25 100%)
ALPHA = 0.5

# FAISS ìŠ¤ì½”ì–´(Inner Product) í™œìš© ì‹œ, 0~1 ì‚¬ì´ ê°’ì´ ë‚˜ì˜¬ ìˆ˜ë„ ìˆê³ 
# BM25 ìŠ¤ì½”ì–´ëŠ” 0~10 ë˜ëŠ” ê·¸ ì´ìƒì¼ ìˆ˜ë„ ìˆìŒ (ë¬¸ì„œ ê¸¸ì´ì— ë”°ë¼ ìƒì´)
# ì‹¤ì œë¡œëŠ” ë‘ ê°’ì„ ì •ê·œí™”í•˜ëŠ” í¸ì´ ì¢‹ìŠµë‹ˆë‹¤(ì˜ˆ: 0~1ë¡œ)
# ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ raw ì ìˆ˜ì—ë§Œ ê³±í•´ì„œ í•©ì‚°

# -----------------------------------------------------------------------------
# 2) .env íŒŒì¼ ë¡œë“œ & OpenAI API ì„¤ì •
# -----------------------------------------------------------------------------
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    raise ValueError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
os.environ["OPENAI_API_KEY"] = api_key

# -----------------------------------------------------------------------------
# 3) Prompt íŒŒì¼ ë¡œë“œ
# -----------------------------------------------------------------------------
prompt_file_path = "src/prompt/prompt.txt"
def load_prompt_from_file(file_path):
    with open(file_path, "r", encoding="utf-8") as file:
        return file.read()

prompt_text = load_prompt_from_file(prompt_file_path)

# -----------------------------------------------------------------------------
# 4) FAISS ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ
#    - Inner Product ëª¨ë“œ ê°•ì œ
# -----------------------------------------------------------------------------
embedding_model = OpenAIEmbeddings(model="text-embedding-3-small")

vectorstore = FAISS.load_local(
    "vdb/faiss_index",
    embeddings=embedding_model,
    allow_dangerous_deserialization=True
)
# metric_type ì„¤ì • (ë‚´ì )
vectorstore.index.metric_type = faiss.METRIC_INNER_PRODUCT

# -----------------------------------------------------------------------------
# 5) BM25 ì¸ë±ìŠ¤ êµ¬ì„± (ì—‘ì…€ì˜ "ì œëª© + ë³¸ë¬¸_ì›ë³¸")
# -----------------------------------------------------------------------------
excel_file = "data_source/ì„¸ë¬´ì‚¬ ë°ì´í„°ì „ì²˜ë¦¬_20250116.xlsx"
df = pd.read_excel(excel_file)

# BM25 ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸: ë¬¸ìì—´ í˜•íƒœ
bm25_documents = df.apply(lambda row: f"{row['ì œëª©']} {row['ë³¸ë¬¸_ì›ë³¸']}", axis=1).tolist()
bm25_tokenized_docs = [doc.split() for doc in bm25_documents]
bm25 = BM25Okapi(bm25_tokenized_docs)

# -----------------------------------------------------------------------------
# 6) í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í•¨ìˆ˜
#    - "BM25 top-k" & "FAISS top-k" => ì ìˆ˜ë¥¼ í•©ì‚°
# -----------------------------------------------------------------------------
def hybrid_search(query: str, k=5):
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì„ ìˆ˜í–‰í•´ (ë¬¸ìì—´, hybrid_score) ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜."""
    tokenized_query = query.split()

    # 1) BM25 ê²€ìƒ‰
    bm25_scores = bm25.get_scores(tokenized_query)
    # ì •ë ¬í•˜ì—¬ ìƒìœ„ k
    # => [(ë¬¸ì„œí…ìŠ¤íŠ¸, bm25_score), ...]
    bm25_top = sorted(
        zip(bm25_documents, bm25_scores),
        key=lambda x: x[1],
        reverse=True
    )[:k]

    # 2) FAISS ê²€ìƒ‰
    # => [(Document, distance), ...]  distance: ë‚´ì ê°’ì— ëŒ€í•œ (1 - similarity) ë˜ëŠ” ìœ ì‚¬
    faiss_results_with_scores = vectorstore.similarity_search_with_score(query, k=k)
    # ì‹¤ì œ langchain_community.vectorstores.FAISS ëŠ” distanceê°€ "1 - cos_sim" í˜•íƒœì¼ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
    # ê·¸ëŸ¬ë‚˜ ìœ„ì—ì„œ metric_type=METRIC_INNER_PRODUCT ë¥¼ ê°•ì œí–ˆìœ¼ë¯€ë¡œ
    # distance == 1 - inner_product or just the raw distance? ì‹¤ì œ êµ¬í˜„ ë”°ë¼ ë‹¤ë¦„
    # ì¼ë‹¨ distanceê°€ "ë‚®ì„ìˆ˜ë¡ ìœ ì‚¬"ë¼ê³  ê°€ì • -> similarity = (1 - distance)
    # (ë§Œì•½ distanceê°€ raw inner productë¼ë©´, ì•„ë˜ ë¡œì§ì„ ì¡°ì •í•´ì•¼ í•©ë‹ˆë‹¤.)

    # 3) í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ í•©ì‚°
    # - doc_score_map: { "ë¬¸ì„œí…ìŠ¤íŠ¸": bm25_score (raw), ... }
    doc_score_map = {}

    # (A) ë¨¼ì € bm25_scoreë¥¼ ì €ì¥(ì•ŒíŒŒ ê³±í•˜ì§€ X)
    for doc_text, bm25_score_value in bm25_top:
        doc_score_map[doc_text] = bm25_score_value

    # (B) FAISS ê²°ê³¼ì™€ í•©ì‚°
    #     Document.page_contentë¥¼ ë¬¸ìì—´ í‚¤ë¡œ ì‚¬ìš©
    for doc_obj, distance_value in faiss_results_with_scores:
        # FAISSê°€ ë‚´ì  ìœ ì‚¬ë„ë¼ê³  ê°€ì •í•˜ë©´,
        # similarity = (1 - distance) (ë§Œì•½ distance=1 - inner_product)
        # ë˜ëŠ” "distance" ìì²´ê°€ inner_product ì ìˆ˜ë©´, similarity=distance
        similarity = 1.0 - distance_value  # ì¼ë‹¨ ì´ëŸ° ì‹ìœ¼ë¡œ ê°€ì •

        doc_text = doc_obj.page_content
        bm25_s = doc_score_map.get(doc_text, 0.0)  # ì—†ìœ¼ë©´ 0
        # ìµœì¢… = alpha * bm25_s + (1-alpha)*similarity
        # ì—¬ê¸°ì„œ alpha=BM25 ë¹„ì¤‘, (1-alpha)=FAISS ë¹„ì¤‘
        final_score = ALPHA * bm25_s + (1.0 - ALPHA) * similarity
        doc_score_map[doc_text] = final_score

    # 4) í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ë¡œ ìƒìœ„ k ì¶”ì¶œ
    # doc_score_map: { "ë¬¸ì„œí…ìŠ¤íŠ¸": ìµœì¢…ì ìˆ˜, ... }
    sorted_docs = sorted(doc_score_map.items(), key=lambda x: x[1], reverse=True)[:k]

    return sorted_docs

# -----------------------------------------------------------------------------
# 7) RAG(LLM QA) êµ¬ì„±
#    - í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì§ì ‘ LLMì— ì „ë‹¬
# -----------------------------------------------------------------------------
prompt = PromptTemplate(
    input_variables=["question", "context"],
    template=prompt_text
)
llm = ChatOpenAI(model_name="gpt-4o", temperature=0)
parser = StrOutputParser()

def generate_answer(question: str, top_k=5):
    """
    1) í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ => ìƒìœ„ kê°œ ë¬¸ì„œ
    2) ë¬¸ì„œë“¤ì„ í•˜ë‚˜ì˜ context ë¬¸ìì—´ë¡œ í•©ì¹¨
    3) LLMì— ì „ë‹¬í•˜ì—¬ ë‹µë³€ ìƒì„±
    """
    hybrid_results = hybrid_search(question, k=top_k)
    if not hybrid_results:
        return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

    # context ìƒì„±: ë¬¸ì„œ ë‚´ìš© + ì ìˆ˜
    context_list = []
    for idx, (doc_text, score) in enumerate(hybrid_results, 1):
        snippet = f"[ë¬¸ì„œ{idx} | ìŠ¤ì½”ì–´={score:.3f}]\n{doc_text}\n"
        context_list.append(snippet)
    context_text = "\n\n".join(context_list)

    # RAG í”„ë¡¬í”„íŠ¸
    prompt_input = {
        "question": question,
        "context": context_text
    }
    # LLM í˜¸ì¶œ
    answer = llm(prompt.format(**prompt_input)).content
    return answer, hybrid_results

# -----------------------------------------------------------------------------
# 8) Streamlit ì•±
# -----------------------------------------------------------------------------
st.set_page_config(page_title="ì„¸ë¬´ì‚¬ ì±—ë´‡", page_icon="ğŸ¤–", layout="wide")

st.title("ğŸ“„ ì„¸ë¬´ì‚¬ ì±—ë´‡ (BM25 + FAISS í•˜ì´ë¸Œë¦¬ë“œ)")
st.write("BM25ì™€ FAISS ì ìˆ˜ë¥¼ ê°€ì¤‘í•©í•˜ì—¬ ìµœì¢… ìƒìœ„ ë¬¸ì„œë¥¼ LLMì— ì „ë‹¬í•©ë‹ˆë‹¤.")

with st.form("chat_form"):
    question = st.text_input(
        "ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:",
        placeholder="ì˜ˆ: ëŒ€í•™ì›ìƒì¸ ë°°ìš°ìê°€ 2024ë…„ 6ì›”ì— ì—°êµ¬ìš©ì—­ë¹„ë¡œ 500ë§Œì›ì„ ë°›ì€ ê²½ìš° ë°°ìš°ìê³µì œê°€ ê°€ëŠ¥í•´?"
    )
    submit_button = st.form_submit_button(label="ì§ˆë¬¸í•˜ê¸°")

if submit_button and question.strip():
    with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
        answer, top_docs = generate_answer(question, top_k=5)

    st.subheader("ğŸ’¡ ìƒì„±ëœ ë‹µë³€")
    st.write(answer)

    st.subheader("ğŸ” ì°¸ì¡°í•œ ë¬¸ì„œ (í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìƒìœ„ 5ê°œ)")
    for idx, (doc_text, score) in enumerate(top_docs, start=1):
        with st.expander(f"ë¬¸ì„œ {idx} | ì ìˆ˜: {score:.3f}"):
            st.write(doc_text[:1000])  # ë¬¸ì„œê°€ ê¸¸ ê²½ìš° ì¼ë¶€ë§Œ
