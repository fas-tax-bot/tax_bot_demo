import os
import faiss
import numpy as np
import pandas as pd
import streamlit as st
from dotenv import load_dotenv
from rank_bm25 import BM25Okapi
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from sklearn.preprocessing import normalize

# ---------------------------------------------------------------------------
# 0)  ìƒìˆ˜ ì„¤ì •
# ---------------------------------------------------------------------------
FETCH_K = 100
TOP_K = 5
THRESHOLD = 0.6  # ì„ê³„ê°’ ìƒìˆ˜

# ---------------------------------------------------------------------------
# 1) .env ì„¤ì •
# ---------------------------------------------------------------------------
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    raise ValueError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
os.environ["OPENAI_API_KEY"] = api_key

# ---------------------------------------------------------------------------
# 2) BM25 ë¡œë”©
# ---------------------------------------------------------------------------
@st.cache_data
def load_bm25_index():
    excel_file = "data_source/ì„¸ë¬´ì‚¬ ë°ì´í„°ì „ì²˜ë¦¬_20250116.xlsx"
    df = pd.read_excel(excel_file)

    # ì›ë³¸ í…ìŠ¤íŠ¸ (ì¤„ë°”ê¿ˆ í¬í•¨)
    bm25_documents = df.apply(lambda row: f"ì œëª©: {row['ì œëª©']}\n\në³¸ë¬¸: {row['ë³¸ë¬¸_ì›ë³¸']}", axis=1).tolist()

    # BM25 í† í°í™”ë¥¼ ìœ„í•´ì„œëŠ” ì¤„ë°”ê¿ˆê³¼ ìƒê´€ì—†ì´ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
    bm25_tokenized_docs = [doc.split() for doc in bm25_documents]
    bm25 = BM25Okapi(bm25_tokenized_docs)

    print(f"âœ… BM25 ë¬¸ì„œ ê°œìˆ˜: {len(bm25_documents)}")

    return bm25, bm25_documents

bm25, bm25_documents = load_bm25_index()

# ---------------------------------------------------------------------------
# 3) FAISS (ì„ë² ë”©) ë¡œë”©
# ---------------------------------------------------------------------------
embedding_model = OpenAIEmbeddings(model="text-embedding-3-small")

@st.cache_resource
def load_embedding_index():
    """
    LangChain Community FAISSë¥¼ ì‚¬ìš©í•˜ì—¬ ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ
    """
    try:
        vectorstore = FAISS.load_local(
            "vdb/faiss_index", embeddings=embedding_model, allow_dangerous_deserialization=True
        )
        print("âœ… ê¸°ì¡´ FAISS ì„ë² ë”© ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ")
    except:
        print("âŒ FAISS ì¸ë±ìŠ¤ íŒŒì¼ ì—†ìŒ, ìƒˆë¡œ ìƒì„± í•„ìš”")
        vectorstore = FAISS.from_documents([], embedding_model)
        vectorstore.save_local("vdb/faiss_index")

    return vectorstore

vectorstore = load_embedding_index()
dense_index = vectorstore.index

# ---------------------------------------------------------------------------
# 4) FAISS Sparse Index (BM25 ì ìˆ˜ ê¸°ë°˜)
# ---------------------------------------------------------------------------
dimension = len(bm25_documents)
sparse_index = faiss.IndexFlatL2(dimension)

bm25_scores_matrix = np.array([bm25.get_scores(doc.split()) for doc in bm25_documents])
bm25_scores_matrix = normalize(bm25_scores_matrix, norm="l2", axis=1)
sparse_index.add(bm25_scores_matrix)

# ---------------------------------------------------------------------------
# 5) Hybrid Search (BM25 + Dense)
# ---------------------------------------------------------------------------
def hybrid_search(query: str, alpha=0.5):
    """
    1) BM25 ì „ì²´ ë¬¸ì„œ ìœ ì‚¬ë„ ê³„ì‚°
    2) Dense ì„ë² ë”© ê²€ìƒ‰ (FETCH_Kê°œ ë¬¸ì„œë¥¼ ê²€ìƒ‰)
    3) ë‘ ìŠ¤ì½”ì–´ë¥¼ min-max ì •ê·œí™”í•œ í›„ ì„ í˜• ê²°í•©í•˜ì—¬ ìµœì¢… ìƒìœ„ ë¬¸ì„œë¥¼ ë°˜í™˜
    """
    tokenized_query = query.split()

    # BM25 ìŠ¤ì½”ì–´ ê³„ì‚° ë° ì •ê·œí™”
    bm25_scores = np.array(bm25.get_scores(tokenized_query))
    bm25_scores_norm = bm25_scores

    # ì„ë² ë”© ê²€ìƒ‰ (FETCH_Kê°œ ë¬¸ì„œ ê²€ìƒ‰)
    query_embedding = embedding_model.embed_query(query)
    D, I = dense_index.search(np.array([query_embedding]), FETCH_K)

    # ê±°ë¦¬ -> ìœ ì‚¬ë„ë¡œ ë³€í™˜ (ìœ ì‚¬ë„ëŠ” [0, 1] ë²”ìœ„ì— ê°€ê¹ê²Œ)
    similarity_scores = 1 - (D / np.max(D))
    similarity_scores = similarity_scores.flatten()

    # BM25ì—ì„œ dense ê²€ìƒ‰ìœ¼ë¡œ ë°˜í™˜ëœ ë¬¸ì„œ ì¸ë±ìŠ¤ì— í•´ë‹¹í•˜ëŠ” BM25 ìŠ¤ì½”ì–´ ì„ íƒ
    selected_bm25_scores = bm25_scores_norm[I[0]]

    # BM25 ìŠ¤ì½”ì–´ì˜ ìŒìˆ˜ ì œê±° (ìµœì†Œê°’ì˜ ì ˆëŒ€ê°’ì„ ë”í•¨)
    min_bm25_score = np.min(selected_bm25_scores)
    selected_bm25_scores = selected_bm25_scores + abs(min_bm25_score)

    # --- ë‘ ìŠ¤ì½”ì–´ì— ëŒ€í•´ min-max ì •ê·œí™” ì§„í–‰ ---
    sb_min = np.min(selected_bm25_scores)
    sb_max = np.max(selected_bm25_scores)
    selected_bm25_scores = (selected_bm25_scores - sb_min) / (sb_max - sb_min + 1e-8)

    sim_min = np.min(similarity_scores)
    sim_max = np.max(similarity_scores)
    similarity_scores = (similarity_scores - sim_min) / (sim_max - sim_min + 1e-8)
    # --------------------------------------------

    # Hybrid ì ìˆ˜ ê³„ì‚° (ë‘ ìŠ¤ì½”ì–´ ëª¨ë‘ [0, 1] ë²”ìœ„)
    hybrid_score = alpha * selected_bm25_scores + (1 - alpha) * similarity_scores
    sorted_indices = np.argsort(-hybrid_score)
    
    final_results = [
        (bm25_documents[I[0][idx]], hybrid_score[idx], selected_bm25_scores[idx], similarity_scores[idx])
        for idx in sorted_indices
    ]
    return final_results

# ---------------------------------------------------------------------------
# 6) RAG(LLM QA) êµ¬ì„±
# ---------------------------------------------------------------------------
prompt_file_path = "src/prompt/prompt.txt"
def load_prompt_from_file(file_path: str) -> str:
    with open(file_path, "r", encoding="utf-8") as f:
        return f.read()

prompt_text = load_prompt_from_file(prompt_file_path)

prompt = PromptTemplate(
    input_variables=["question", "context"],
    template=prompt_text
)

llm = ChatOpenAI(model_name="gpt-4o", temperature=0)
parser = StrOutputParser()

def generate_answer(question: str):
    """
    Hybrid Search í›„ ìƒìœ„ TOP_Kê°œì˜ ë¬¸ì„œë¥¼ contextë¡œ í•˜ì—¬ GPT-4ì— ì „ì†¡
    """
    hybrid_results = hybrid_search(question)
    
    # GPTì— ì „ë‹¬í•  ì»¨í…ìŠ¤íŠ¸ ìƒì„± (THRESHOLD ì´ìƒ ë¬¸ì„œë§Œ í¬í•¨)
    top_docs = [doc for doc in hybrid_results[:TOP_K] if doc[1] >= THRESHOLD]

    if not top_docs:
        # ğŸ”¹ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆì„ ë•Œ GPTì—ê²Œ ê¸°ë³¸ ë©”ì‹œì§€ë¥¼ ì „ë‹¬í•˜ë„ë¡ ì„¤ì •
        context_text = "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë„ˆê°€ ì•„ëŠ” ë‚´ìš©ìœ¼ë¡œ ëŒ€ë‹µí•´ì¤˜"
    else:
        # ğŸ”¹ ì •ìƒì ìœ¼ë¡œ ë¬¸ì„œë¥¼ ì°¾ì•˜ì„ ê²½ìš° ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ìƒì„±
        context_list = []
        for idx, (doc_text, score, bm25_score, faiss_score) in enumerate(top_docs, start=1):
            snippet = f"[ë¬¸ì„œ {idx} | Hybrid ì ìˆ˜={score:.3f} | BM25={bm25_score:.3f} | FAISS={faiss_score:.3f}]\n{doc_text}\n"
            context_list.append(snippet)
        context_text = "\n\n".join(context_list)
    
    # ğŸ”¹ Prompt ìƒì„± ë° GPT-4 í˜¸ì¶œ
    prompt_input = {"question": question, "context": context_text}
    final_prompt = prompt.format(**prompt_input)
    result = llm.invoke(final_prompt)
    answer = result.content

    return answer, top_docs  # í•„í„°ë§ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜


# ---------------------------------------------------------------------------
# 7) Streamlit UI + ê²€ìƒ‰ ê²°ê³¼ ì—‘ì…€ ì €ì¥
# ---------------------------------------------------------------------------
st.title("ğŸ“„ ì„¸ë¬´ì‚¬ ì±—ë´‡")
st.write("ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ AIê°€ ë¬¸ì„œë¥¼ ì°¸ì¡°í•˜ì—¬ ë‹µë³€ë“œë¦½ë‹ˆë‹¤.")

with st.form("chat_form"):
    question = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:", placeholder="ì˜ˆ: ë°°ìš°ìê°€ ì—°êµ¬ìš©ì—­ë¹„ë¥¼ ë°›ì€ ê²½ìš° ë°°ìš°ìê³µì œê°€ ê°€ëŠ¥í•©ë‹ˆê¹Œ?")
    submit_button = st.form_submit_button(label="ì§ˆë¬¸í•˜ê¸°")

if submit_button and question.strip():
    with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
        answer, hybrid_results = generate_answer(question)

    st.subheader("ğŸ’¡ ìƒì„±ëœ ë‹µë³€")
    st.write(answer)

    # TOP_K ë¬¸ì„œ ì¤‘ ì„ê³„ê°’ ì´ìƒì˜ hybrid ì ìˆ˜ë¥¼ ê°€ì§„ ë¬¸ì„œë§Œ í•„í„°ë§
    filtered_docs = [doc for doc in hybrid_results[:TOP_K] if doc[1] >= THRESHOLD]
    if filtered_docs:
        st.subheader("ğŸ” ì°¸ì¡°í•œ ë¬¸ì„œ")
        for idx, (doc_text, score, bm25_score, faiss_score) in enumerate(filtered_docs, start=1):
            with st.expander(f"ë¬¸ì„œ {idx} | Hybrid ì ìˆ˜: {score:.3f}"):
                st.write(doc_text[:2000])  # ë¬¸ì„œê°€ ê¸¸ ê²½ìš° ì¼ë¶€ë§Œ ì¶œë ¥
                
    # # ê²€ìƒ‰ëœ ëª¨ë“  ë¬¸ì„œë¥¼ ì—‘ì…€ë¡œ ì €ì¥
    # import re
    # safe_question = re.sub(r'[\\/:*?"<>|]', '_', question)
    # desktop_path = os.path.join(os.path.expanduser("~"), "ë°”íƒ•í™”ë©´")
    # excel_filename = f"{safe_question}.xlsx"
    # save_path = os.path.join(desktop_path, "RAG_ì—‘ì…€", excel_filename)

    # df = pd.DataFrame({
    #     "ì§ˆë¬¸": [question] * len(filtered_docs),
    #     "ë¬¸ì„œë‚´ìš©": [doc for (doc, _, _, _) in filtered_docs],
    #     "Hybrid ì ìˆ˜": [hybrid for (_, hybrid, _, _) in filtered_docs],
    #     "BM25 ì ìˆ˜": [bm25 for (_, _, bm25, _) in filtered_docs],
    #     "FAISS ìœ ì‚¬ë„ ì ìˆ˜": [faiss for (_, _, _, faiss) in filtered_docs],
    # })

    # df.to_excel(save_path, index=False)
    # st.success(f"ê²€ìƒ‰ëœ ë¬¸ì„œ ì „ì²´ë¥¼ ì—‘ì…€ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤: {save_path}")

